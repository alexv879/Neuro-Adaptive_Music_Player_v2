# README Update - Project Structure Section

**Add this section to the existing README.md after the "Overview" section**

---

## ğŸ“ Project Structure

### Directory Overview

```
Neuro-Adaptive Music Player v2/
â”‚
â”œâ”€â”€ ğŸ“‚ src/                          Core library modules
â”‚   â”œâ”€â”€ config.py                    âš™ï¸  System configuration
â”‚   â”œâ”€â”€ eeg_preprocessing.py         ğŸ”§ Signal filtering & artifact removal
â”‚   â”œâ”€â”€ eeg_features.py              ğŸ“Š Feature extraction (band power, FAA)
â”‚   â”œâ”€â”€ emotion_recognition_model.py ğŸ§  CNN+BiLSTM deep learning
â”‚   â”œâ”€â”€ music_recommendation.py      ğŸµ Music engine (Spotify/YouTube/local)
â”‚   â”œâ”€â”€ llm_music_recommender.py     ğŸ¤– LLM-powered recommendations
â”‚   â”œâ”€â”€ data_loaders.py              ğŸ“‚ DEAP/SEED dataset loaders
â”‚   â””â”€â”€ __init__.py                  ğŸ“¦ Package initialization
â”‚
â”œâ”€â”€ ğŸ“‚ examples/                     Hands-on tutorials
â”‚   â”œâ”€â”€ README.md                    ğŸ“– Examples guide (START HERE)
â”‚   â”œâ”€â”€ 01_complete_pipeline.py      Full EEG-to-music demo
â”‚   â””â”€â”€ 02_llm_recommendation...     LLM integration demo
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                        Unit & integration tests
â”‚   â”œâ”€â”€ test_preprocessing.py
â”‚   â”œâ”€â”€ test_features.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                         Detailed documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md              System design & algorithms
â”‚   â”œâ”€â”€ READABILITY_ANALYSIS.md      Code quality analysis
â”‚   â”œâ”€â”€ REFACTORING_ROADMAP.md       Improvement plan
â”‚   â””â”€â”€ QUICK_REFERENCE.md           Code standards cheat sheet
â”‚
â”œâ”€â”€ ğŸ“‚ models/                       ğŸ’¾ Saved model weights
â”œâ”€â”€ ğŸ“‚ data/                         ğŸ“ EEG datasets (DEAP, SEED)
â”œâ”€â”€ ğŸ“‚ logs/                         ğŸ“ Application logs
â”œâ”€â”€ ğŸ“‚ music_cache/                  ğŸµ Cached music metadata
â”‚
â”œâ”€â”€ README.md                        You are here! ğŸ‘‹
â”œâ”€â”€ requirements.txt                 Python dependencies
â”œâ”€â”€ .env.example                     Environment variable template
â””â”€â”€ LICENSE                          Usage terms

```

### ğŸ¯ Quick Navigation

**New to the project?**
1. Read this README for overview
2. Check [examples/README.md](examples/README.md) for hands-on tutorials
3. See [ARCHITECTURE.md](docs/ARCHITECTURE.md) for technical details

**Researchers?**
- [ARCHITECTURE.md](docs/ARCHITECTURE.md) - Scientific background & references
- [tests/](tests/) - Validation & benchmarks
- Research papers cited throughout code docstrings

**Developers?**
- [QUICK_REFERENCE.md](docs/QUICK_REFERENCE.md) - Code standards
- [REFACTORING_ROADMAP.md](docs/REFACTORING_ROADMAP.md) - Improvement plan
- [CONTRIBUTING.md](CONTRIBUTING.md) - Development guide

---

## ğŸš€ Quick Start (5 Minutes)

### Step 1: Install Dependencies
```bash
# Clone repository
git clone https://github.com/alexv879/Neuro-Adaptive_Music_Player_v2.git
cd "Neuro-Adaptive Music Player v2"

# Install core dependencies
pip install numpy scipy pandas matplotlib

# Optional: Deep learning (for emotion recognition)
pip install tensorflow>=2.10.0

# Optional: LLM integration
pip install openai python-dotenv

# Optional: Music playback
pip install spotipy pygame
```

### Step 2: Run Your First Example
```bash
# Basic preprocessing demonstration
python examples/01_complete_pipeline.py --mode simulated
```

**Expected output:** Preprocessed EEG signals, feature extraction, emotion prediction

### Step 3: Try LLM-Powered Music Recommendation
```bash
# Set OpenAI API key
export OPENAI_API_KEY="sk-your-key-here"

# Or create .env file:
echo "OPENAI_API_KEY=sk-your-key-here" > .env

# Run LLM recommendation demo
python examples/02_llm_recommendation_pipeline.py --mode simulated
```

**Expected output:** AI-generated music recommendations based on detected emotions

---

## ğŸ“Š Module Import Guide

### Preprocessing
```python
from src.eeg_preprocessing import EEGPreprocessor

preprocessor = EEGPreprocessor(fs=256)
clean_data = preprocessor.preprocess(raw_eeg, apply_notch=True)
```

### Feature Extraction
```python
from src.eeg_features import EEGFeatureExtractor

extractor = EEGFeatureExtractor(fs=256)
features = extractor.extract_all_features(clean_data, channel_names)
feature_vector = extractor.features_to_vector(features)
```

### Emotion Recognition
```python
from src.emotion_recognition_model import EmotionRecognitionModel

model = EmotionRecognitionModel(input_shape=(167,), n_classes=5)
model.build_model(architecture='cnn_bilstm')
model.train(X_train, y_train, X_val, y_val)
predictions = model.predict(X_test)
```

### Music Recommendation
```python
from src.music_recommendation import MusicRecommendationEngine, MusicPlatform

engine = MusicRecommendationEngine(platform=MusicPlatform.SPOTIFY)
tracks = engine.recommend(emotion, n_tracks=5)
engine.play(tracks[0])
```

### LLM Integration
```python
from src.llm_music_recommender import LLMMusicRecommender

llm = LLMMusicRecommender(api_key="sk-...", model="gpt-4o")
recommendations = llm.recommend(
    mood_tag="happy and energetic",
    confidence=0.85,
    n_tracks=3
)
```

---

## ğŸ“– Documentation Index

| Document | Purpose | Audience |
|----------|---------|----------|
| [README.md](README.md) | Project overview, installation, quick start | Everyone |
| [examples/README.md](examples/README.md) | Tutorial path, learning progression | New users |
| [ARCHITECTURE.md](docs/ARCHITECTURE.md) | System design, algorithms, references | Researchers, developers |
| [READABILITY_ANALYSIS.md](docs/READABILITY_ANALYSIS.md) | Code quality review, recommendations | Developers |
| [REFACTORING_ROADMAP.md](docs/REFACTORING_ROADMAP.md) | Implementation plan for improvements | Contributors |
| [QUICK_REFERENCE.md](docs/QUICK_REFERENCE.md) | Code standards cheat sheet | Developers |
| [CONTRIBUTING.md](CONTRIBUTING.md) | How to contribute, git workflow | Contributors |

---

## ğŸ“ Learning Paths

### For Students (New to EEG/ML)
1. **Week 1:** Understanding EEG signals
   - Read [ARCHITECTURE.md](docs/ARCHITECTURE.md) introduction
   - Run `examples/01_complete_pipeline.py --mode simulated`
   - Understand preprocessing steps

2. **Week 2:** Feature extraction
   - Learn about frequency bands (delta, theta, alpha, beta, gamma)
   - Experiment with frontal alpha asymmetry (FAA)
   - Visualize features with matplotlib

3. **Week 3:** Machine learning
   - Train emotion classifier on simulated data
   - Understand CNN+BiLSTM architecture
   - Evaluate model performance

4. **Week 4:** Complete pipeline
   - Integrate all components
   - Test with real DEAP dataset
   - Experiment with LLM recommendations

### For Researchers (Implementing Papers)
1. Review [ARCHITECTURE.md](docs/ARCHITECTURE.md) for algorithm references
2. Check existing feature implementations in `src/eeg_features.py`
3. Add custom features following patterns in code
4. Benchmark against DEAP/SEED datasets
5. Compare results with published papers

### For Developers (Adding Features)
1. Read [QUICK_REFERENCE.md](docs/QUICK_REFERENCE.md) for code standards
2. Check [REFACTORING_ROADMAP.md](docs/REFACTORING_ROADMAP.md) for current work
3. Follow patterns in existing modules
4. Add tests for new features
5. Submit pull request with documentation

---

## ğŸ—ºï¸ System Architecture (High-Level)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER INPUT                              â”‚
â”‚                    (EEG Signals / Datasets)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EEG PREPROCESSING                            â”‚
â”‚  â€¢ Bandpass filtering (0.5-45 Hz)                              â”‚
â”‚  â€¢ Notch filter (50/60 Hz powerline)                           â”‚
â”‚  â€¢ Artifact detection & removal                                â”‚
â”‚                                                                 â”‚
â”‚  Module: src/eeg_preprocessing.py                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  FEATURE EXTRACTION                             â”‚
â”‚  â€¢ Band power (delta, theta, alpha, beta, gamma)               â”‚
â”‚  â€¢ Frontal alpha asymmetry (FAA)                               â”‚
â”‚  â€¢ Statistical features (mean, std, skewness, kurtosis)        â”‚
â”‚  â€¢ Spectral entropy                                            â”‚
â”‚                                                                 â”‚
â”‚  Module: src/eeg_features.py                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               EMOTION RECOGNITION (Deep Learning)               â”‚
â”‚  â€¢ CNN: Spatial-frequency feature extraction                   â”‚
â”‚  â€¢ BiLSTM: Temporal dependency modeling                        â”‚
â”‚  â€¢ Hierarchical classification (valence â†’ emotion)             â”‚
â”‚  â€¢ Output: Emotion label + confidence                          â”‚
â”‚                                                                 â”‚
â”‚  Module: src/emotion_recognition_model.py                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            LLM MUSIC RECOMMENDATION (Optional)                  â”‚
â”‚  â€¢ Dynamic prompt generation from emotion + confidence          â”‚
â”‚  â€¢ GPT-4/GPT-4o creative recommendations                       â”‚
â”‚  â€¢ Context-aware (time, activity, history)                     â”‚
â”‚  â€¢ Fallback to rule-based if API unavailable                   â”‚
â”‚                                                                 â”‚
â”‚  Module: src/llm_music_recommender.py                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 MUSIC PLAYBACK ENGINE                           â”‚
â”‚  â€¢ Platform selection (Spotify / YouTube / Local)               â”‚
â”‚  â€¢ Track search and retrieval                                   â”‚
â”‚  â€¢ Playback control                                             â”‚
â”‚  â€¢ Recommendation history tracking                              â”‚
â”‚                                                                 â”‚
â”‚  Module: src/music_recommendation.py                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      USER OUTPUT                                â”‚
â”‚          (Music Playback / Recommendations / Logs)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pipeline Timing:**
- Preprocessing: ~15ms per 5-second window
- Feature extraction: ~18ms per window
- Model inference: <10ms per prediction
- **Total latency:** <50ms (real-time capable at 20 Hz)

---

## ğŸ”¬ Scientific Background

This project implements algorithms from peer-reviewed research:

### Preprocessing
- **Gramfort et al. (2013)** - MNE-Python methodology
- **Delorme & Makeig (2004)** - EEGLAB pipeline
- **Kothe & Makeig (2013)** - Clean Rawdata plugin

### Feature Extraction
- **Frantzidis et al. (2010)** - Frontal Alpha Asymmetry
- **Zheng & Lu (2015)** - Differential Entropy features (84% accuracy on SEED)
- **Welch (1967)** - Power spectral density estimation

### Deep Learning
- **Lawhern et al. (2018)** - EEGNet architecture
- **Yang et al. (2018)** - Hierarchical emotion classification
- **Li et al. (2018)** - LSTM for temporal EEG modeling

Full citations in [ARCHITECTURE.md](docs/ARCHITECTURE.md)

---

## ğŸ¤ Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for:
- Code standards ([QUICK_REFERENCE.md](docs/QUICK_REFERENCE.md))
- Git workflow
- Testing requirements
- Pull request process

**Current focus areas:**
1. Splitting large files (>700 lines) into submodules
2. Adding more unit tests (target: 90% coverage)
3. Implementing ICA artifact correction
4. Adding more dataset loaders (SEED++, AMIGOS)

See [REFACTORING_ROADMAP.md](docs/REFACTORING_ROADMAP.md) for detailed improvement plan.

---

## ğŸ“Š Code Quality

- **Documentation:** â­â­â­â­â­ (Excellent NumPy-style docstrings)
- **Modularity:** â­â­â­â­â­ (Clean separation of concerns)
- **Type Hints:** â­â­â­â­â˜† (Most functions typed)
- **Test Coverage:** â­â­â­â˜†â˜† (70%, targeting 90%)
- **Performance:** â­â­â­â­â­ (Vectorized, <50ms latency)

See [READABILITY_ANALYSIS.md](docs/READABILITY_ANALYSIS.md) for detailed assessment.

---

## ğŸ“ Code Examples

### Complete Pipeline (Simplified)
```python
import numpy as np
from src.eeg_preprocessing import EEGPreprocessor
from src.eeg_features import EEGFeatureExtractor
from src.emotion_recognition_model import EmotionRecognitionModel
from src.llm_music_recommender import LLMMusicRecommender

# 1. Load EEG data
eeg_data = np.load('your_eeg.npy')  # Shape: (n_channels, n_samples)

# 2. Preprocess
preprocessor = EEGPreprocessor(fs=256)
clean_data = preprocessor.preprocess(eeg_data, apply_notch=True)

# 3. Extract features
extractor = EEGFeatureExtractor(fs=256)
features = extractor.extract_all_features(clean_data, channel_names)
feature_vec = extractor.features_to_vector(features)

# 4. Predict emotion
model = EmotionRecognitionModel.load('models/pretrained.h5')
emotion, confidence = model.predict(feature_vec)

# 5. Get LLM music recommendations
llm = LLMMusicRecommender(api_key=os.getenv('OPENAI_API_KEY'))
tracks = llm.recommend(
    mood_tag=emotion.value,
    confidence=confidence,
    n_tracks=3
)

# 6. Display recommendations
for i, track in enumerate(tracks, 1):
    print(f"{i}. {track.artist} - {track.title}")
    print(f"   Reasoning: {track.reasoning}\n")
```

---

## ğŸ¯ Future Roadmap

### v2.1 (Next Release)
- [ ] Split large files into submodules (see [REFACTORING_ROADMAP.md](docs/REFACTORING_ROADMAP.md))
- [ ] Add more examples with progressive complexity
- [ ] Increase test coverage to 90%
- [ ] Implement ICA artifact correction
- [ ] Add SEED dataset loader

### v2.2 (Planned)
- [ ] Real-time streaming mode with hardware support
- [ ] Transfer learning for personalization
- [ ] Web dashboard for visualization
- [ ] Mobile app (Android/iOS)
- [ ] Cross-dataset validation toolkit

### v3.0 (Research)
- [ ] Transformer-based architecture
- [ ] Multi-modal input (EEG + ECG + EMG)
- [ ] Self-supervised pre-training
- [ ] Explainable AI for emotion classification
- [ ] Cloud deployment (AWS/Azure/GCP)

---

**Remember:** 
- Check [examples/README.md](examples/README.md) for hands-on tutorials
- See [ARCHITECTURE.md](docs/ARCHITECTURE.md) for technical deep dive
- Review [QUICK_REFERENCE.md](docs/QUICK_REFERENCE.md) for code standards

**Have questions?** Open an issue on [GitHub](https://github.com/alexv879/Neuro-Adaptive_Music_Player_v2/issues)

